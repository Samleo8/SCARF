<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <title>SCARF</title>
    <link href="https://bootswatch.com/5/journal/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
    <link rel="stylesheet" href="css/style.css">
</head>

<body>
    <div class="header">
        <div class="title">SCARF: Stereo Cross-Attention Radiance Fields</div>
        <div class="subtitle">Using a cross-attention mechanism for rendering in Stereo Radiance Fields
            <a href="#cite-1" class="cite">[1]</a></div>
    </div>

    <h1 id="sec-intro">Introduction</h1>
    <p>
        In Stereo Cross Attention Radiance Fields, we explore adding transformer-based techniques to Stereo Radiance
        Fields<a href="#cite-1" class="cite">[1]</a>. In recent years, there have been great advances in transformer
        techniques which allow for faster training and consistent architecture across models. (INSERT CITATION HERE).
        Although transformer based techniques have already been applied to the original NeRF
        paper,<a href="#cite-2" class="cite">[2]</a><a href="#cite-3" class="cite">[3]</a> we aim to apply it to the SRF
        paper which takes advantage of
    </p>

    <figure>
        <img src="images/srf_architecture.png">
        <div class="caption">Figure 1: SRF architecture diagram<a href="#cite-1" class="cite">[1]</a></div>
    </figure>

    <figure>
        <img src="images/architecture.png">
        <div class="caption">Figure 2: Our architecture diagram. The top portion of the diagram is taken from the SRF
            paper. <a href="#cite-1" class="cite">[1]</a></div>
    </figure>


    <h1>Methods</h1>
    <p>
        First, we investigated the architecture from the SRF paper shown in Figure <a class="figref">1</a>. We
        discovered that the
        image
        encoding pairing, unsupervised stereo module, multi-view extraction, and correspondence encoding could all be
        replaced with a transformer. This drastically simplified our architecture shown in Figure
        <a class="figref">2</a>.
    </p>

    <p>
        In order to reduce model size, we added an image feature compression layer, which reduces the size of the image
        features. We opt to go with a compression fully connected network instead of reducing the complexity of the
        feature detection since this both reprojects into a space to help the transformer learn, and also allows for
        some learning of relation between multiscale features before the transformer. We experimented both with and
        without the feature compression layer (see results) and found that the network performed much better with it.
    </p>

    <p>
        After feature compression, we use transformers to learn the relation between different views. We experimented
        with different numbers of cross attention layers and heads per layer.
    </p>

    <p>
        Finally, we pass the output of the transformer layer through the fully connected "Radiance Field Decoder", which
        results in an RGBÏƒ value for each 3D point.
    </p>

    <h1>Results</h1>

    <table>
        <tr>
            <td>
                <figure>
                    <img src="images/render23_2L_8H.png">
                    <div class="caption">Figure 3: Renering of scene 23, with compression, 2 layers of 8 heads cross attention
                        layers.</div>
                </figure>
            </td>
            <td>
                <figure>
                    <img src="images/render23_2L_16H.png">
                    <div class="caption">Figure 4: Renering of scene 23, with compression, 2 layers of 16 heads cross attention
                        layers.</div>
                </figure>
            </td>
            <td>
                <figure>
                    <img src="images/render23_2L_32H.png">
                    <div class="caption">Figure 5: Renering of scene 23, with compression, 2 layers of 32 heads cross attention
                        layers.</div>
                </figure>
            </td>
            <td>
                <figure>
                    <img src="images/render23_2L_32H.png">
                    <div class="caption">Figure 5: Renering of scene 23, with compression, 2 layers of 32 heads cross attention
                        layers.</div>
                </figure>
            </td>
            <td>
                <figure>
                    <img src="images/render23_4L_16H.png">
                    <div class="caption">Figure 6: Renering of scene 23, with compression, 4 layers of 16 heads cross attention
                        layers.</div>
                </figure>
            </td>
            <td>
                <figure>
                    <img src="images/no_compression.png">
                    <div class="caption">Figure 7: Renering of scene 23, with no compression.</div>
                </figure>
            </td>

        </tr>

    </table>
    

    

    <h1>References</h1>

    <ol>
        <li id="cite-1">Chibane, Julian, et al. 'Stereo Radiance Fields (SRF): Learning View Synthesis from Sparse Views
            of Novel Scenes'. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 2021.</li>
        <li id="cite-2">Mildenhall, Ben, et al. 'NeRF: Representing Scenes as Neural Radiance Fields for View
            Synthesis'. CoRR, vol. abs/2003.08934, 2020, https://arxiv.org/abs/2003.08934.</li>
        <li id="cite-3">Some random NeRF + tranformer paper</li>
    </ol>

    <!-- Modal -->
    <div id="modal" class="modal" tabindex="-1">
        <div class="modal-dialog modal-xl">
            <div class="modal-content">
                <div class="modal-header">
                    <h5 class="modal-title"></h5>
                    <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                </div>
                <div class="modal-body">
                    <img src="" class="modal-image img-fluid" style="width: 100%" />
                </div>
                    <div class="modal-footer">
                        <button type="button" class="btn btn-primary" data-bs-dismiss="modal">Close</button>
                    </div>
                </div>
            </div>
        </div>

        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous">
        </script>
        <script>
            // Populate model information
            var _modal = new bootstrap.Modal(document.getElementById("modal"))
            var _modal_title = document.getElementsByClassName("modal-title")
            var _modal_image = document.getElementsByClassName("modal-image")
            var _images = document.querySelectorAll("figure img")
            for (var i = 0; i < _images.length; i++) {

                (function(index) {
                    var title = _images[index].parentNode.children[1].innerHTML
                    var image = _images[index].src;
                    _images[index].addEventListener("click", function() {
                        for (var j = 0; j < _modal_title.length; j++) {
                            _modal_title[j].innerHTML = title
                        }

                        _modal_image[0].src = image
                        _modal.show()
                    })
                })(i)

            }

            // Populate figure IDs
            var _figures = document.getElementsByTagName("figure")

            for (var i = 0; i < _figures.length; i++) {
                _figures[i].id = "fig-" + i
            }

            // Popular 

        </script>
</body>

</html>
