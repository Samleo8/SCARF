<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <title>SCARF</title>
    <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" crossorigin="anonymous"> -->
    <script src="https://kit.fontawesome.com/d9a8590a21.js" crossorigin="anonymous"></script>
    <link href="https://bootswatch.com/5/journal/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
    <link rel="stylesheet" href="css/style.css">
</head>

<body>
    <div class="float-triangle">
        <a href="https://github.com/Samleo8/SCARF" target="_blank" class="btn">
            <span class="fa fa-github"></span>
        </a>
    </div>

    <div class="header">
        <div class="title">SCARF: Stereo Cross-Attention Radiance Fields</div>
        <div class="subtitle">Using a cross-attention mechanism for rendering in Stereo Radiance Fields
            <a href="#cite-1" class="cite">[1]</a></div>

        <div class="authors">
            <div class="author-names">Samuel Leong*, Alex Strasser*</div>
            <div class="author-affiliations">Carnegie Mellon University</div>
            <div class="author-emails">{<a href="mailto:scleong@andrew.cmu.edu" target="_blank">scleong</a>,
                <a href="mailto:astrasse@andrew.cmu.edu" target="_blank">astrasse</a>}@andrew.cmu.edu
            </div>

            <div class="footnote">* Equal contribution</div>
        </div>

        <div class="links">
            <a href="https://github.com/Samleo8/SCARF" target="_blank" class="btn btn-primary large-font">
            <span class="fa fa-github"></span>&nbsp;&nbsp;GitHub</a>
            <a href="
            https://docs.google.com/presentation/d/1BXCPMn_tfyXVVvL5m5Y0Ear06_MKOi1l6b6xrLuzZBk/edit#slide=id.p" target="_blank" class="btn btn-primary large-font">
            <span class="fa fa-person-chalkboard"></span>&nbsp;&nbsp;Presentation</a>
            <a href="#results" class="btn btn-primary large-font">
            <span class="fa fa-images"></span>&nbsp;&nbsp;Results</a>
        </div>

        <div class="teaser gallery" fignum="0">
            <div class="gallery-row d-flex flex-row flex-wrap">
                <div class="gallery-item p-2">
                    <figure>
                        <img src="images/nerf_baseline.png">
                        <div class="caption">NERF Baseline</div>
                    </figure>
                </div>
                <div class="gallery-item p-2">
                    <figure>
                        <img src="images/srf_baseline.png">
                        <div class="caption">SRF Baseline</div>
                    </figure>
                </div>
                <div class="gallery-item p-2">
                    <figure>
                        <img src="images/render23_2L_32H.png">
                        <div class="caption">Ours (<a href="#fig-4">2L 32 H</a>)</div>
                    </figure>
                </div>
            </div>
            <div class="footnote">*Baselines are artificially downsampled by 4 for fair comparison against our model
                which renders a downsampled version due to compute constraints. For full-res baseline images, see the
                <a href="https://github.com/jchibane/srf" target="_blank">SRF GitHub Repo</a></div>
        </div>
    </div>

    <div class="body">
        <h1 id="sec-intro">Introduction</h1>
        <p>
            In Stereo Cross-Attention Radiance Fields (SCARF), we explore adding attention-based techniques to Stereo
            Radiance Fields <a href="#cite-1" class="cite">[1]</a>. In recent years, there have been great advances in
            attention/transformer-based techniques which allow for faster training and consistent architecture across
            models.
            Although transformer-based techniques have already been applied to the original NeRF
            paper <a href="#cite-2" class="cite">[2]</a> <a href="#cite-3" class="cite">[3]</a>, we aim to apply it to
            the SRF paper, which, instead of learning the scene, aims to emulate and learn multi-view stereo. Their
            method allows for training with sparse views (only 10 instead of 100), and also learns how to perform
            multi-view stereo correspondences rather than learning only a single scene, thus allowing for rendering of
            novel scenes in a single pass (with fine-tuning).
        </p>

        <figure>
            <img src="images/srf_architecture.png">
            <div class="caption">Figure 1: SRF architecture diagram. <a href="#cite-1" class="cite">[1]</a></div>
        </figure>

        <figure>
            <img src="images/architecture.png">
            <div class="caption">Figure 2: SCARF architecture diagram. The top portion of the diagram is taken directly
                from the
                SRF paper. <a href="#cite-1" class="cite">[1]</a></div>
        </figure>


        <h1 id="methods">Methods</h1>
        <p>
            First, we investigated the architecture from the SRF paper shown in Figure <a class="figref">1</a>. We
            hypothesized that the feature pairing, unsupervised stereo module, multi-view extraction, and
            correspondence encoding could all be
            replaced by a (cross-)attention mechanism. Practically, this was implemented in PyTorch using a transformer
            encoder,
            borrowing from the ideas of the Vision Transformer (ViT) <a href="#cite-4" class="cite">[4]</a>. This makes
            intuitive sense, because stereo is about finding correspondences between different features in the image,
            the process of which can be reframed as cross-attending across features. This drastically
            simplified our architecture shown in Figure
            <a class="figref">2</a>.
        </p>

        <p>
            After the image features layer, we first feed into an image feature compression network. This helped reduce
            the number of parameters that the transformer network needed to learn. See
            <a href="#results-1">Experiment 1</a> under the <a href="#results">Results</a> section for justification and
            different techniques that were attempted.
        </p>

        <p>
            After feature compression, we feed a sequence of feature vectors (one per view) into a transformer encoder,
            which aims to learn the relation between different views. The feature vectors were positionally encoded in
            the
            same way as ViT <a href="#cite-4" class="cite">[4]</a> to track which image each feature set came from. We
            experimented with various different hyperparameters for this transformer encoder to get good results. We
            attempted different numbers of heads in <a href="#results-2">Experiment 2</a> and different number of
            transformer layers in <a href="#results-3">Experiment 3</a>. Max-pooling is run over the output images to
            preserve the most important features from each image.
        </p>

        <p>
            Finally, we pass the output of the transformer layer through the fully connected "Radiance Field Decoder"
            (2 FC layers), which results in an RGB-&sigma; value for each 3D point.
        </p>

        <h1 id="results">Results</h1>

        <h2 id="results-1">Experiment 1: Feature Compression</h2>

        <p>
            First, we experimented with two different techniques in order to reduce the model size. First, we attempted
            to use a reduced image feature extraction network. This reduced network used half the number of channels for
            each layer, and dropped the last two convolution layers from the SRF paper. This significantly reduced the
            number of parameters that the transformer needed to learn.
        </p>

        <p>
            The other technique we used to reduce model size was an image feature compression layer, which reduces the
            size of the image features. This technique both reprojects into a space to help the transformer learn, and
            also allows for some learning of relation between multi-scale features before the transformer encoder.
        </p>

        <div class="gallery" fignum="3">
            <div class="gallery-row d-flex flex-row flex-wrap">
                <div class="gallery-item p-2">
                    <figure>
                        <img src="images/render23_2L_8H_nocompress.png">
                        <div class="caption">Figure 3a: 2 layers, 8 heads, with no compression.</div>
                    </figure>
                </div>
                <div class="gallery-item p-2">
                    <figure>
                        <img src="images/render23_2L_16H_reduced.png">
                        <div class="caption">Figure 3b: 2 layers, 16 heads, with reduced feature network.</div>
                    </figure>
                </div>
                <div class="gallery-item p-2">
                    <figure>
                        <img src="images/render23_2L_8H.png">
                        <div class="caption">Figure 3c: 2 layers, 8 heads, with compression.</div>
                    </figure>
                </div>
            </div>
            <div class="collapse" id="video-row-1">
                <div class="gallery-row d-flex flex-row flex-wrap">
                    <div class="gallery-item p-2">
                        <video controls>
                            <source src="videos/render23_2L_nocompress.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="gallery-item p-2">
                        <video controls>
                            <source src="videos/render23_2L_16H_reduced.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="gallery-item p-2">
                        <video controls>
                            <source src="videos/render23_2L_8H.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
            <div class="caption">
                Figure 3: Gallery of results for rendering of Scene 23 of the DTU Dataset
                <a href="#cite-5" class="cite">[5]</a> with different feature reduction techniques.

                <button class="btn btn-primary" type="button" data-bs-toggle="collapse" data-bs-target="#video-row-1" aria-expanded="false" aria-controls="video-row-1">Toggle Time Lapse</button>

                <div class="footnote">
                    NOTE: All rendered images are downscaled by 4 because of compute constraints. Time lapses downscaled
                    by 16.
                </div>
            </div>
        </div>

        <p>
            Our first experiment was to see the trade-offs between no compression, reduced features, and image
            compression. We found that the network was not able to learn quickly with the large transformer size. It did
            not learn a significant amount of information in the scene by iteration 50k, as shown in Figure
            <a class="figref">3a</a>. Between feature compression and network reduction, we found that the feature
            compression was able to do a much better job at learning quickly, reconstructing finer details like color,
            and minimizing artifacts
        </p>

        <h2 id="results-2">Experiment 2: Transformer Heads</h2>

        <p>
            For the transformer, we first varied the nubmer of heads used in each cross attention layer. We held the
            number of layers constant at 2, and attempted 8, 16, and 32 heads.
        </p>

        <div class="gallery" fignum="4">
            <div class="gallery-row d-flex flex-row flex-wrap">
                <div class="gallery-item p-2">
                    <figure>
                        <img src="images/render23_2L_8H.png">
                        <div class="caption">Figure 4a: 2 layers, 8 heads</div>
                    </figure>
                </div>
                <div class="gallery-item p-2">
                    <figure>
                        <img src="images/render23_2L_16H.png">
                        <div class="caption">Figure 4b: 2 layers, 16 heads</div>
                    </figure>
                </div>
                <div class="gallery-item p-2">
                    <figure>
                        <img src="images/render23_2L_32H.png">
                        <div class="caption">Figure 4c: 2 layers, 32 heads</div>
                    </figure>
                </div>
            </div>
            <div class="collapse" id="video-row-2">
                <div class="gallery-row d-flex flex-row flex-wrap">
                    <div class="gallery-item p-2">
                        <video controls>
                            <source src="videos/render23_2L_8H.mp4">
                        </video>
                    </div>
                    <div class="gallery-item p-2">
                        <video controls>
                            <source src="videos/render23_2L_16H.mp4">
                        </video>
                    </div>
                    <div class="gallery-item p-2">
                        <video controls>
                            <source src="videos/render23_2L_32H.mp4">
                        </video>
                    </div>
                </div>
            </div>
            <div class="caption flex-row">
                Figure 4: Gallery of results for rendering of Scene 23 of the DTU Dataset
                <a href="#cite-5" class="cite">[5]</a> with varied number of transformer heads.
                <div></div>
                Images are generated with compression and without feature reduction.

                <button class="btn btn-primary" type="button" data-bs-toggle="collapse" data-bs-target="#video-row-2" aria-expanded="false" aria-controls="video-row-2">Toggle Time Lapse</button>

                <div class="footnote">
                    NOTE: All rendered images are downscaled by 4 because of compute constraints. Time lapses downscaled
                    by 16.
                </div>
            </div>
        </div>

        <p>
            As can be seen in Figure <a class="figref">4</a>, increasing the number of heads was significant in
            improving the quality of the results, in particular the detail and vibrancy of colors. For example, the
            rendering with only 8 heads
            (Figure <a class="figref">4a</a>) looks faded, but the rendering with 16 heads (Figure
            <a class="figref">4b</a>) looks much more vibrant. We begin to see distinct colors for the different
            houses and the umbrellas on the patio are yellow as they should be. However, some of the colors still
            seem faded, and there are some "blurring" artifacts near the patio area where the umbrellas are.
        </p>
        <p>
            The rendering with 32 heads (Figure <a class="figref">4c</a>) looks even better, and is our best result
            overall. The house and patio colors look much more vibrant and closer to the ground truth. There is also
            increased detail for the windows, roofs and other objects in the scene. While the model is larger and
            takes longer to train on its own, using pretrained weights from previous heads allowed us to reduce
            training time significantly.
        </p>

        <p>
            The results make intuitive sense, because increasing the number of heads increases the number of feature
            subspaces to cross-attend to. This may allow the model to learn more complex and different relationships
            between features, and therefore increases the level of detail and color vibrancy in the rendered scene.
        </p>

        <h2>Experiment 3: Transformer Layers</h2>

        <p>
            We then varied the nubmer of transformer cross attention layers. We held the number of heads constant at
            16, and attempted 2 and 4 layers.
        </p>

        <div class="gallery" fignum="5">
            <div class="gallery-row d-flex flex-row flex-wrap">
                <div class="gallery-item p-2">
                    <figure>
                        <img src="images/render23_2L_16H.png">
                        <div class="caption">Figure 5a: 2 layers, 16 heads.</div>
                    </figure>
                </div>
                <div class="gallery-item p-2">
                    <figure>
                        <img src="images/render23_4L_16H.png">
                        <div class="caption">Figure 5b: 4 layers, 16 heads</div>
                    </figure>
                </div>
            </div>
            <div class="collapse" id="video-row-3">
                <div class="gallery-row d-flex flex-row flex-wrap">
                    <div class="gallery-item p-2">
                        <video controls>
                            <source src="videos/render23_2L_16H.mp4">
                        </video>
                    </div>
                    <div class="gallery-item p-2">
                        <video controls>
                            <source src="videos/render23_4L_16H.mp4">
                        </video>
                    </div>
                </div>
            </div>
            <div class="caption flex-row">
                Figure 5: Gallery of results for rendering of Scene 23 of the DTU
                Dataset <a href="#cite-5" class="cite">[5]</a> with varied number of transformer layers.
                <div></div>
                Images are generated with compression and without feature reduction.

                <button class="btn btn-primary" type="button" data-bs-toggle="collapse" data-bs-target="#video-row-3" aria-expanded="false" aria-controls="video-row-3">Toggle Time Lapse</button>


                <div class="footnote">
                    NOTE: All rendered images are downscaled by 4 because of compute constraints. Time lapses downscaled by 16.
                </div>
            </div>
        </div>

        <p>
            Finally, we added more layers to the transformer. We transfered the weights from the 2 layer 8 head
            model
            and found that the network performed even better with 4 layers. The performance of the 4 layer 16
            head model is comparable to the performance of the 2 layer 32 head model.
        </p>

        <h1 id="future-work">Future Work</h1>

        <p>
            More experimentation is needed to understand the following:
            <ul>
                <li>Effect of positional encoding on the results</li>
                <li>Full retrain without transfer learning to compare training time to SRF paper</li>
                <li>Longer training times on larger networks to see if performance further improves</li>
                <li>Alternatives for max pooling as a transformer reduction operation</li>
            </ul>
        </p>

        <h1 id="bonus-content">Bonus Content</h1>
        <p>You can view compiled time-lapse animations/videos of the training processes for experiments by clicking the
            red buttons near the captions. The button below toggles visibility for all of them.
            This is applicable for Figures <a class="figref">3</a>, <a class="figref">4</a> and <a class="figref">5</a>
            above.
            <button class="btn btn-primary" type="button" data-bs-toggle="collapse" data-bs-target=".collapse" aria-expanded="false" aria-controls="video-row-1 video-row-2 video-row-3">Toggle Time Lapse</button>
        </p>
    </div>


    <div class="references">
        <h1 id="references">References</h1>

        <ol>
            <li id="cite-1">Chibane, Julian, et al. 'Stereo Radiance Fields (SRF): Learning View Synthesis from
                Sparse
                Views
                of Novel Scenes'. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 2021.
            </li>
            <li id="cite-2">Mildenhall, Ben, et al. 'NeRF: Representing Scenes as Neural Radiance Fields for
                View
                Synthesis'. CoRR, vol. abs/2003.08934, 2020, https://arxiv.org/abs/2003.08934.</li>
            <li id="cite-3">Reizenstein, Jeremy, et al. "Common objects in 3d: Large-scale learning and
                evaluation
                of
                real-life 3d category reconstruction." Proceedings of the IEEE/CVF International Conference on
                Computer
                Vision. 2021.</li>
            <li id="cite-4">Ranftl, Ren√©, Alexey Bochkovskiy, and Vladlen Koltun. "Vision transformers for dense
                prediction." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.</li>
            <li id="cite-5">Jensen, Rasmus, et al. "Large scale multi-view stereopsis evaluation." Proceedings
                of
                the
                IEEE
                conference on computer vision and pattern recognition. 2014.</li>
        </ol>

    </div>

    <!-- Modal -->
    <div id="modal" class="modal" tabindex="-10">
        <div class="modal-dialog modal-xl">
            <div class="modal-content">
                <div class="modal-header">
                    <h5 class="modal-title"></h5>
                    <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                </div>
                <div class="modal-body">
                    <img class="modal-image img-fluid" style="width:100%" src="" />
                </div>
                    <div class="modal-footer">
                        <button type="button" class="btn btn-primary" data-bs-dismiss="modal">Close</button>
                    </div>
                </div>
            </div>
        </div>

        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
            integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous">
        </script>
        <script>
            // Populate model information
            var _modal = new bootstrap.Modal(document.getElementById("modal"));
            var _modal_title = document.getElementsByClassName("modal-title");
            var _modal_image = document.getElementsByClassName("modal-image");
            var _images = document.querySelectorAll("figure img");
            for (var i = 0; i < _images.length; i++) {

                (function(index) {
                    var title = _images[index].parentNode.children[1].innerHTML;
                    var image = _images[index].src;
                    _images[index].addEventListener("click", function() {
                        for (var j = 0; j < _modal_title.length; j++) {
                            _modal_title[j].innerHTML = title;
                        }

                        _modal_image[0].src = image;
                        _modal.show();
                    })
                })(i)

            }

            // Populate figure IDs
            var _figures = document.getElementsByTagName("figure");

            var id = 1;
            for (var i = 0; i < _figures.length; i++) {
                var fig = _figures[i];

                // Ignore figures that are part of a gallery
                if (!fig.parentNode.className.startsWith("gallery-item")) {
                    fig.id = "fig-" + id;
                    id++;
                }
            }

            // Populate gallery ids
            var _galleries = document.getElementsByClassName("gallery");
            for (var i = 0; i < _galleries.length; i++) {
                var gal = _galleries[i];
                var id = gal.getAttribute("fignum");
                gal.id = "fig-" + id;
            }

            // Populate references to images
            var _figrefs = document.getElementsByClassName("figref")
            for (var i = 0; i < _figrefs.length; i++) {
                var _figref = _figrefs[i];

                var figId = _figref.innerHTML.replace(/[A-Za-z]+/g, '');

                _figref.href = "#fig-" + figId;
            }

        </script>
</body>

</html>
